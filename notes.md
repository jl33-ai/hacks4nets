

https://viso.ai/deep-learning/adversarial-machine-learning/


    // x2. bc I am, standards, and: backprop? make them hIGHLY DESCIRPTIVE. let go; Fight. It was just: i need to fdo many hi worlds this weekedn; ml, go, c_+


1. vnn

- fast gradient sign

2. image 

- sai 
- negative prompting 

3. llm

- andrej
- dan

4. traditional

- data polluting

5. graph NN's

- from the talk?


1. **Fast Gradient Sign Method (FGSM)**: A simple yet effective method that creates adversarial examples by perturbing the original input data in the direction of the gradient of the loss with respect to the input.

2. **Basic Iterative Method (BIM)**: An extension of FGSM, BIM applies the gradient update iteratively with small steps, providing more control over the perturbation process.

3. **Jacobian-based Saliency Map Attack (JSMA)**: This technique focuses on modifying a few pixels that have the most significant impact on the classification, determined by the Jacobian matrix of derivatives.

4. **Carlini & Wagner Attacks (C&W Attacks)**: A powerful and sophisticated adversarial attack method that formulates the problem as an optimization task with specific constraints to minimize the perturbation.

5. **DeepFool**: An algorithm designed to efficiently compute adversarial perturbations by iteratively moving the input towards the nearest class boundary.

6. **Universal Adversarial Perturbations (UAP)**: This technique creates image-agnostic perturbations that can be applied to any image to fool the classifier.

7. **One Pixel Attack**: An attack method that demonstrates the vulnerability of deep neural networks by altering only one pixel in the input.

8. **Patch Attacks**: These involve creating a specific, often conspicuous patch that, when placed in the scene, causes the model to misclassify the input.

9. **L-BFGS Attack**: An early method of generating adversarial examples using the L-BFGS (Limited-memory Broyden–Fletcher–Goldfarb–Shanno) optimization algorithm.

10. **Boundary Attack**: A decision-based attack that starts from a large adversarial perturbation and then iteratively reduces its magnitude until it finds the decision boundary.

11. **Zeroth Order Optimization (ZOO) Attack**: A black-box attack strategy that approximates the gradient of a neural network solely based on input-output pairs, making it effective against models with hidden gradients.

12. **HopSkipJumpAttack**: A decision-based, black-box attack that estimates gradients to generate adversarial examples with minimal changes.

13. **Projected Gradient Descent (PGD)**: Considered one of the strongest first-order attacks, PGD applies iterative gradient-based updates to craft adversarial examples within a specified norm ball.

14. **Elastic-Net Attacks to DNNs (EAD)**: Combines L1 and L2 distance metrics to create adversarial examples, providing a balance between perturbation visibility and effectiveness.

15. **Feature Adversaries**: An attack focusing on making deep representations of an adversarial example similar to those of a target image, causing misclassification.

16. **Autoencoder-based Attacks**: Utilizes an autoencoder architecture to generate adversarial examples, often used in scenarios where the adversary has limited knowledge of the target model.

17. **GAN-based Attacks**: Leveraging Generative Adversarial Networks to create adversarial examples, often resulting in more realistic perturbations.


million dollar questoins
- can LLM's improve by talking to themselves (alphaGo ting)
- gallale